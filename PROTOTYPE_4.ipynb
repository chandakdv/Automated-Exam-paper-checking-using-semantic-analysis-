{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUdnyz_KHBkS",
        "outputId": "84c6fe49-0236-4932-ca96-53ad23fff089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.6.0-py3-none-any.whl (163 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/163.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m153.6/163.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m153.6/163.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.1/163.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1iTWWNbF9a6",
        "outputId": "928b24e1-e455-462b-daeb-3fcad4ebc5ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for file: ml_dataset.csv\n",
            "                                            Question  \\\n",
            "0             Decision Tree Classification Algorithm   \n",
            "1                          What is Machine Learning?   \n",
            "2           What is overfitting in machine learning?   \n",
            "3  What is regularization, and why is it importan...   \n",
            "4  What is cross-validation, and why is it import...   \n",
            "\n",
            "                              Similarity (SBERT)  \n",
            "0   [90.3779, 91.5018, 90.311, 86.8643, 91.0256]  \n",
            "1  [89.7051, 88.2936, 81.7708, 91.9873, 87.5651]  \n",
            "2  [91.9654, 89.0456, 90.8505, 91.1234, 92.6987]  \n",
            "3    [84.0593, 90.5002, 89.774, 90.9812, 92.249]  \n",
            "4  [96.2854, 89.6283, 78.1682, 87.7823, 90.0192]  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the dataset folder in your Google Drive\n",
        "dataset_folder_path = '/content/drive/My Drive/Dataset/'\n",
        "\n",
        "# Get a list of CSV files in the dataset folder\n",
        "csv_files = [f for f in os.listdir(dataset_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Process each CSV file\n",
        "for csv_file in csv_files:\n",
        "    # Load CSV file\n",
        "    df = pd.read_csv(os.path.join(dataset_folder_path, csv_file))\n",
        "\n",
        "    # Process only the first 5 questions\n",
        "    results = []\n",
        "    question_count = 0\n",
        "\n",
        "    # SBERT for Similarity\n",
        "    sbert_model = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        if question_count >= 5:\n",
        "            break  # Exit the loop if we have processed 5 questions\n",
        "\n",
        "        question = row['Question']\n",
        "        sample_answers = [row['Ans 1'], row['Ans 2'], row['Ans 3'], row['Ans 4'], row['Ans 5']]\n",
        "        model_answer = row['Ans 6']\n",
        "\n",
        "        # Encode model answer\n",
        "        model_answer_embedding_sbert = sbert_model.encode(str(model_answer))\n",
        "\n",
        "        # Calculate cosine similarity between each sample answer and model answer using SBERT\n",
        "        similarity_scores_sbert = []\n",
        "        for sample_answer in sample_answers:\n",
        "            sample_answer_embedding_sbert = sbert_model.encode(str(sample_answer))\n",
        "            similarity_score = cosine_similarity([model_answer_embedding_sbert], [sample_answer_embedding_sbert])[0][0]\n",
        "            similarity_scores_sbert.append(round((similarity_score + 1) * 50, 4))  # Scaling to 0-100 range and rounding to 4 decimal places\n",
        "\n",
        "        # Append results for this question to the list\n",
        "        results.append({\n",
        "            'Question': question,\n",
        "            'Similarity (SBERT)': similarity_scores_sbert\n",
        "        })\n",
        "\n",
        "        question_count += 1\n",
        "\n",
        "    # Display results for the current CSV file\n",
        "    print(f\"Results for file: {csv_file}\")\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the dataset folder in your Google Drive\n",
        "dataset_folder_path = '/content/drive/My Drive/Dataset/'\n",
        "\n",
        "# Get a list of CSV files in the dataset folder\n",
        "csv_files = [f for f in os.listdir(dataset_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Process each CSV file\n",
        "for csv_file in csv_files:\n",
        "    # Load CSV file\n",
        "    df = pd.read_csv(os.path.join(dataset_folder_path, csv_file))\n",
        "\n",
        "    # Process only the first 5 questions\n",
        "    results = []\n",
        "    question_count = 0\n",
        "\n",
        "    # SBERT for Similarity\n",
        "    sbert_model = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        if question_count >= 5:\n",
        "            break  # Exit the loop if we have processed 5 questions\n",
        "\n",
        "        question = row['Question']\n",
        "        sample_answers = [row[f'Ans {i}'] for i in range(1, 9)]  # Assuming columns are Ans 1 to Ans 8\n",
        "        model_answer = row['Ans 8']  # Assuming the model answer is in column Ans 9\n",
        "\n",
        "        # Encode model answer\n",
        "        model_answer_embedding_sbert = sbert_model.encode(str(model_answer))\n",
        "\n",
        "        # Calculate cosine similarity between each sample answer and model answer using SBERT\n",
        "        similarity_scores_sbert = []\n",
        "        for sample_answer in sample_answers:\n",
        "            sample_answer_embedding_sbert = sbert_model.encode(str(sample_answer))\n",
        "            similarity_score = cosine_similarity([model_answer_embedding_sbert], [sample_answer_embedding_sbert])[0][0]\n",
        "            similarity_scores_sbert.append(round((similarity_score + 1) * 50, 4))  # Scaling to 0-100 range and rounding to 4 decimal places\n",
        "\n",
        "        # Append results for this question to the list\n",
        "        results.append({\n",
        "            'Question': question,\n",
        "            'Similarity (SBERT)': similarity_scores_sbert\n",
        "        })\n",
        "\n",
        "        question_count += 1\n",
        "\n",
        "    # Display results for the current CSV file\n",
        "    print(f\"Results for file: {csv_file}\")\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3ET8C_4G8KB",
        "outputId": "31184000-b4a9-40c6-da17-30fb7dde3f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for file: Updated_ml_dataset.csv\n",
            "                                            Question  \\\n",
            "0             Decision Tree Classification Algorithm   \n",
            "1                          What is Machine Learning?   \n",
            "2           What is overfitting in machine learning?   \n",
            "3  What is regularization, and why is it importan...   \n",
            "4  What is cross-validation, and why is it import...   \n",
            "\n",
            "                                  Similarity (SBERT)  \n",
            "0  [90.3779, 91.5018, 90.311, 86.8643, 91.0256, 6...  \n",
            "1  [89.7051, 88.2936, 81.7708, 91.9873, 87.5651, ...  \n",
            "2  [91.9654, 89.0456, 90.8505, 91.1234, 92.6987, ...  \n",
            "3  [84.0593, 90.5002, 89.774, 90.9812, 92.249, 64...  \n",
            "4  [96.2854, 89.6283, 78.1682, 87.7823, 90.0192, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the dataset folder in your Google Drive\n",
        "dataset_folder_path = '/content/drive/My Drive/Dataset/'\n",
        "\n",
        "# Get a list of CSV files in the dataset folder\n",
        "csv_files = [f for f in os.listdir(dataset_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Process each CSV file\n",
        "for csv_file in csv_files:\n",
        "    # Load CSV file\n",
        "    df = pd.read_csv(os.path.join(dataset_folder_path, csv_file))\n",
        "\n",
        "    # Process only the first 5 questions\n",
        "    results = []\n",
        "    question_count = 0\n",
        "\n",
        "    # SBERT for Similarity\n",
        "    sbert_model = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        if question_count >= 5:\n",
        "            break  # Exit the loop if we have processed 5 questions\n",
        "\n",
        "        question = row['Question']\n",
        "        sample_answers = [row[f'Ans {i}'] for i in range(1, 9)]  # Assuming columns are Ans 1 to Ans 8\n",
        "        model_answer = row['Ans 8']  # Assuming the model answer is in column Ans 9\n",
        "\n",
        "        # Encode model answer\n",
        "        model_answer_embedding_sbert = sbert_model.encode(str(model_answer))\n",
        "\n",
        "        # Calculate cosine similarity between each sample answer and model answer using SBERT\n",
        "        similarity_scores_sbert = []\n",
        "        for sample_answer in sample_answers:\n",
        "            sample_answer_embedding_sbert = sbert_model.encode(str(sample_answer))\n",
        "            similarity_score = cosine_similarity([model_answer_embedding_sbert], [sample_answer_embedding_sbert])[0][0]\n",
        "            similarity_scores_sbert.append(round((similarity_score + 1) * 50, 4))  # Scaling to 0-100 range and rounding to 4 decimal places\n",
        "\n",
        "        # Append results for this question to the list\n",
        "        results.append({\n",
        "            'Question': question,\n",
        "            'Similarity (SBERT)': similarity_scores_sbert\n",
        "        })\n",
        "\n",
        "        question_count += 1\n",
        "\n",
        "    # Display results for the current CSV file\n",
        "    print(f\"Results for file: {csv_file}\")\n",
        "    results_df = pd.DataFrame(results)\n",
        "    # Set option to display the full list without truncation\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "    print(results_df)\n",
        "    # Reset option to its default value for future display\n",
        "    pd.reset_option('display.max_colwidth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDBaw7fVZFHF",
        "outputId": "efcad2f4-b342-4d7c-f79b-0c778a0168d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for file: Updated_ml_dataset.csv\n",
            "                                                                 Question  \\\n",
            "0                                  Decision Tree Classification Algorithm   \n",
            "1                                               What is Machine Learning?   \n",
            "2                                What is overfitting in machine learning?   \n",
            "3    What is regularization, and why is it important in machine learning?   \n",
            "4  What is cross-validation, and why is it important in machine learning?   \n",
            "\n",
            "                                                       Similarity (SBERT)  \n",
            "0   [90.3779, 91.5018, 90.311, 86.8643, 91.0256, 69.3496, 95.2497, 100.0]  \n",
            "1  [89.7051, 88.2936, 81.7708, 91.9873, 87.5651, 60.0664, 94.7592, 100.0]  \n",
            "2  [91.9654, 89.0456, 90.8505, 91.1234, 92.6987, 63.8095, 97.9082, 100.0]  \n",
            "3    [84.0593, 90.5002, 89.774, 90.9812, 92.249, 64.8345, 97.6008, 100.0]  \n",
            "4  [96.2854, 89.6283, 78.1682, 87.7823, 90.0192, 63.0639, 94.7794, 100.0]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the dataset folder in your Google Drive\n",
        "dataset_folder_path = '/content/drive/My Drive/Dataset/'\n",
        "\n",
        "# Get a list of CSV files in the dataset folder\n",
        "csv_files = [f for f in os.listdir(dataset_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Load SpaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process each CSV file\n",
        "for csv_file in csv_files:\n",
        "    # Load CSV file\n",
        "    df = pd.read_csv(os.path.join(dataset_folder_path, csv_file))\n",
        "\n",
        "    # Process only the first 5 questions\n",
        "    results = []\n",
        "    question_count = 0\n",
        "\n",
        "    # SBERT for Similarity\n",
        "    sbert_model = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        if question_count >= 5:\n",
        "            break  # Exit the loop if we have processed 5 questions\n",
        "\n",
        "        question = row['Question']\n",
        "        sample_answers = [row[f'Ans {i}'] for i in range(1, 9)]  # Assuming columns are Ans 1 to Ans 8\n",
        "        model_answer = row['Ans 8']  # Assuming the model answer is in column Ans 9\n",
        "\n",
        "        # Encode model answer\n",
        "        model_answer_embedding_sbert = sbert_model.encode(str(model_answer))\n",
        "\n",
        "        # Calculate cosine similarity between each sample answer and model answer using SBERT\n",
        "        similarity_scores_sbert = []\n",
        "        for sample_answer in sample_answers:\n",
        "            sample_answer_embedding_sbert = sbert_model.encode(str(sample_answer))\n",
        "\n",
        "            # Check for negation context in the sample answer\n",
        "            negation_context = False\n",
        "            doc = nlp(sample_answer)\n",
        "            for token in doc:\n",
        "                if token.dep_ == 'neg':\n",
        "                    negation_context = True\n",
        "                    break\n",
        "\n",
        "            # Adjust similarity score based on negation context\n",
        "            if negation_context:\n",
        "                similarity_score = 1 - cosine_similarity([model_answer_embedding_sbert], [sample_answer_embedding_sbert])[0][0]\n",
        "            else:\n",
        "                similarity_score = cosine_similarity([model_answer_embedding_sbert], [sample_answer_embedding_sbert])[0][0]\n",
        "\n",
        "            similarity_scores_sbert.append(round(similarity_score * 100, 4))  # Scaling to 0-100 range and rounding to 4 decimal places\n",
        "\n",
        "        # Append results for this question to the list\n",
        "        results.append({\n",
        "            'Question': question,\n",
        "            'Similarity (SBERT)': similarity_scores_sbert\n",
        "        })\n",
        "\n",
        "        question_count += 1\n",
        "\n",
        "    # Display results for the current CSV file\n",
        "    print(f\"Results for file: {csv_file}\")\n",
        "    results_df = pd.DataFrame(results)\n",
        "    # Set option to display the full list without truncation\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "    print(results_df)\n",
        "    # Reset option to its default value for future display\n",
        "    pd.reset_option('display.max_colwidth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DlqdFeYf98y",
        "outputId": "c07511f6-be62-4d12-9903-31a5376668dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for file: Updated_ml_dataset.csv\n",
            "                                                                 Question  \\\n",
            "0                                  Decision Tree Classification Algorithm   \n",
            "1                                               What is Machine Learning?   \n",
            "2                                What is overfitting in machine learning?   \n",
            "3    What is regularization, and why is it important in machine learning?   \n",
            "4  What is cross-validation, and why is it important in machine learning?   \n",
            "\n",
            "                                                       Similarity (SBERT)  \n",
            "0    [19.2442, 83.0036, 80.622, 73.7286, 82.0512, 38.6992, 9.5007, 100.0]  \n",
            "1  [79.4102, 76.5871, 63.5415, 83.9745, 75.1302, 20.1329, 10.4816, 100.0]  \n",
            "2   [16.0692, 21.9089, 18.2989, 17.7532, 14.6025, 27.6189, 4.1835, 100.0]  \n",
            "3    [68.1186, 18.9995, 79.5481, 18.0376, 84.4981, 29.669, 4.7985, 100.0]  \n",
            "4  [92.5707, 20.7434, 43.6636, 24.4355, 80.0385, 26.1278, 10.4412, 100.0]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the dataset folder in your Google Drive\n",
        "dataset_folder_path = '/content/drive/My Drive/Dataset/'\n",
        "\n",
        "# Get a list of CSV files in the dataset folder\n",
        "csv_files = [f for f in os.listdir(dataset_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Load SpaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process each CSV file\n",
        "for csv_file in csv_files:\n",
        "    # Load CSV file\n",
        "    df = pd.read_csv(os.path.join(dataset_folder_path, csv_file))\n",
        "\n",
        "    # Process only the first 5 questions\n",
        "    results = []\n",
        "    question_count = 0\n",
        "\n",
        "    # SBERT for Similarity\n",
        "    sbert_model = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        if question_count >= 5:\n",
        "            break  # Exit the loop if we have processed 5 questions\n",
        "\n",
        "        question = row['Question']\n",
        "        sample_answers = [row[f'Ans {i}'] for i in range(1, 9)]  # Assuming columns are Ans 1 to Ans 8\n",
        "        model_answer = row['Ans 8']  # Assuming the model answer is in column Ans 9\n",
        "\n",
        "        # Encode model answer\n",
        "        model_answer_embedding_sbert = sbert_model.encode(str(model_answer))\n",
        "\n",
        "        # Calculate cosine similarity between each sample answer and model answer using SBERT\n",
        "        similarity_scores_sbert = []\n",
        "        for sample_answer in sample_answers:\n",
        "            sample_answer_embedding_sbert = sbert_model.encode(str(sample_answer))\n",
        "\n",
        "            # Check for negation context in the sample answer\n",
        "            negation_context = False\n",
        "            doc = nlp(sample_answer)\n",
        "            for token in doc:\n",
        "                if token.dep_ == 'neg' and any(child.dep_ == 'neg' for child in token.children):\n",
        "                    negation_context = True\n",
        "                    break\n",
        "\n",
        "            # Adjust similarity score based on negation context\n",
        "            if negation_context:\n",
        "                similarity_score = 1 - cosine_similarity([model_answer_embedding_sbert], [sample_answer_embedding_sbert])[0][0]\n",
        "            else:\n",
        "                similarity_score = cosine_similarity([model_answer_embedding_sbert], [sample_answer_embedding_sbert])[0][0]\n",
        "\n",
        "            similarity_scores_sbert.append(round(similarity_score * 100, 4))  # Scaling to 0-100 range and rounding to 4 decimal places\n",
        "\n",
        "        # Append results for this question to the list\n",
        "        results.append({\n",
        "            'Question': question,\n",
        "            'Similarity (SBERT)': similarity_scores_sbert\n",
        "        })\n",
        "\n",
        "        question_count += 1\n",
        "\n",
        "    # Display results for the current CSV file\n",
        "    print(f\"Results for file: {csv_file}\")\n",
        "    results_df = pd.DataFrame(results)\n",
        "    # Set option to display the full list without truncation\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "    print(results_df)\n",
        "    # Reset option to its default value for future display\n",
        "    pd.reset_option('display.max_colwidth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkoEMTTvf-Zm",
        "outputId": "47335658-1101-4680-d611-537079e5485f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for file: Updated_ml_dataset - Updated_ml_dataset.csv\n",
            "                                                                 Question  \\\n",
            "0                                  Decision Tree Classification Algorithm   \n",
            "1                                               What is Machine Learning?   \n",
            "2                                What is overfitting in machine learning?   \n",
            "3    What is regularization, and why is it important in machine learning?   \n",
            "4  What is cross-validation, and why is it important in machine learning?   \n",
            "\n",
            "                                                       Similarity (SBERT)  \n",
            "0   [80.7558, 83.0036, 80.622, 73.7286, 82.0512, 18.5068, 90.4993, 100.0]  \n",
            "1  [79.4102, 76.5871, 63.5415, 83.9745, 75.1302, 31.9764, 89.5184, 100.0]  \n",
            "2  [83.9308, 78.0911, 81.7011, 82.2468, 85.3975, 20.0504, 95.8165, 100.0]  \n",
            "3   [68.1186, 81.0005, 79.5481, 81.9624, 84.4981, 31.493, 95.2015, 100.0]  \n",
            "4  [92.5707, 79.2566, 56.3364, 75.5645, 80.0385, 29.8136, 89.5588, 100.0]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the dataset folder in your Google Drive\n",
        "dataset_folder_path = '/content/drive/My Drive/Dataset/'\n",
        "\n",
        "# Get a list of CSV files in the dataset folder\n",
        "csv_files = [f for f in os.listdir(dataset_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Load a pre-trained SpaCy English model for sentence processing\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process each CSV file\n",
        "for csv_file in csv_files:\n",
        "    # Load CSV file\n",
        "    df = pd.read_csv(os.path.join(dataset_folder_path, csv_file))\n",
        "\n",
        "    # Process only the first 5 questions\n",
        "    results = []\n",
        "    question_count = 0\n",
        "\n",
        "    # SBERT for Similarity\n",
        "    sbert_model = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        if question_count >= 5:\n",
        "            break  # Exit the loop if we have processed 5 questions\n",
        "\n",
        "        question = row['Question']\n",
        "        sample_answers = [row[f'Ans {i}'] for i in range(1, 9)]  # Assuming columns are Ans 1 to Ans 8\n",
        "        model_answer = row['Ans 8']  # Assuming the model answer is in column Ans 9\n",
        "\n",
        "        # Encode model answer\n",
        "        model_answer_embedding_sbert = sbert_model.encode(str(model_answer))\n",
        "\n",
        "        # Calculate cosine similarity between each sample answer and model answer using SBERT\n",
        "        similarity_scores_sbert = []\n",
        "        for sample_answer in sample_answers:\n",
        "            sample_answer_embedding_sbert = sbert_model.encode(str(sample_answer))\n",
        "\n",
        "            # Calculate cosine similarity between the model answer and sample answer\n",
        "            similarity_before_negation = cosine_similarity([model_answer_embedding_sbert], [sample_answer_embedding_sbert])[0][0]\n",
        "\n",
        "            # Check if negation changes the meaning\n",
        "            sample_answer_negated = \"not \" + sample_answer\n",
        "            sample_answer_embedding_sbert_negated = sbert_model.encode(str(sample_answer_negated))\n",
        "            similarity_after_negation = cosine_similarity([model_answer_embedding_sbert], [sample_answer_embedding_sbert_negated])[0][0]\n",
        "\n",
        "            # Adjust similarity score based on negation context\n",
        "            if similarity_after_negation < similarity_before_negation:\n",
        "                similarity_score = similarity_after_negation\n",
        "            else:\n",
        "                similarity_score = similarity_before_negation\n",
        "\n",
        "            similarity_scores_sbert.append(round(similarity_score * 100, 4))  # Scaling to 0-100 range and rounding to 4 decimal places\n",
        "\n",
        "        # Append results for this question to the list\n",
        "        results.append({\n",
        "            'Question': question,\n",
        "            'Similarity (SBERT)': similarity_scores_sbert\n",
        "        })\n",
        "\n",
        "        question_count += 1\n",
        "\n",
        "    # Display results for the current CSV file\n",
        "    print(f\"Results for file: {csv_file}\")\n",
        "    results_df = pd.DataFrame(results)\n",
        "    # Set option to display the full list without truncation\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "    print(results_df)\n",
        "    # Reset option to its default value for future display\n",
        "    pd.reset_option('display.max_colwidth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfEoCE3QlIOz",
        "outputId": "0842b8a8-61f9-461c-cdea-f6a2221233a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for file: Updated_ml_dataset - Updated_ml_dataset.csv\n",
            "                                                                 Question  \\\n",
            "0                                  Decision Tree Classification Algorithm   \n",
            "1                                               What is Machine Learning?   \n",
            "2                                What is overfitting in machine learning?   \n",
            "3    What is regularization, and why is it important in machine learning?   \n",
            "4  What is cross-validation, and why is it important in machine learning?   \n",
            "\n",
            "                                                         Similarity (SBERT)  \n",
            "0  [78.6504, 81.9209, 77.5888, 71.5434, 79.9735, 18.0933, 89.4874, 98.6471]  \n",
            "1  [78.0563, 74.2784, 62.3828, 81.7758, 74.4795, 30.6774, 88.5098, 98.8951]  \n",
            "2  [83.1031, 78.0289, 81.7011, 81.3737, 84.6148, 19.4635, 94.4269, 99.1221]  \n",
            "3   [67.6407, 80.0438, 79.4339, 81.7409, 84.3394, 31.4452, 94.187, 99.0345]  \n",
            "4   [91.9717, 77.777, 55.2789, 73.7162, 78.8623, 27.8006, 88.1886, 98.5201]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sl_kXelfl6Dp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}